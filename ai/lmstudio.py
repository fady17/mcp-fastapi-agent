# main.py (v6.0 - Mission-Aligned and Definitive)

"""
FastAPI Chat Orchestration Layer for LM Studio

This application acts as a secure, state-managing backend for a chatbot UI.
It receives chat messages, orchestrates calls to a pre-configured LM Studio
instance, and returns the model's response. It correctly handles tool-use
conversations initiated by LM Studio without needing to know the tool
definitions in advance, adhering to the original mission.

Version 6.0: Re-aligned with the core mission. This version removes the
unnecessary `tools` parameter from the request, relying on LM Studio's
pre-configured tools (e.g., MCP). It is both functionally correct and
statically type-safe.
"""

import os
import logging
from typing import List, Optional, cast, Dict, Any

import uvicorn
from fastapi import FastAPI, HTTPException, status
from pydantic import BaseModel, Field, ValidationError
from openai import OpenAI, APIConnectionError
from openai.types.chat import ChatCompletionMessageParam

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
LM_STUDIO_BASE_URL = os.getenv("LM_STUDIO_BASE_URL", "http://localhost:1234/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "local-model") # LM Studio will use the loaded model

# --- Pydantic Models (API Contract - Simplified and Robust) ---

# These models handle the full conversation lifecycle, including tool calls
# generated by the pre-configured LM Studio server.
class ToolCallFunction(BaseModel):
    name: str
    arguments: str

class ToolCall(BaseModel):
    id: str
    type: str
    function: ToolCallFunction

class ChatMessage(BaseModel):
    role: str
    content: Optional[str] = None
    # Present when the assistant requests a tool call.
    tool_calls: Optional[List[ToolCall]] = None
    # Present for a `role: tool` message, indicating which call it's a response to.
    tool_call_id: Optional[str] = Field(None)

class ChatRequest(BaseModel):
    """The request no longer needs a `tools` parameter, as per the mission."""
    history: List[ChatMessage] = Field(..., description="A list of previous messages in the conversation.")
    new_message: str = Field(..., description="The new text message sent by the user.")

class ChatResponse(BaseModel):
    history: List[ChatMessage] = Field(..., description="The complete, updated conversation history.")


# --- FastAPI Application Setup ---
app = FastAPI(
    title="Chat Orchestration Layer for LM Studio",
    description="A simple, robust, and type-safe API that proxies chat requests to a pre-configured LM Studio instance.",
    version="6.0.0"
)

client = OpenAI(base_url=LM_STUDIO_BASE_URL, api_key="not-needed")
logging.info(f"Initialized OpenAI client for LM Studio at {LM_STUDIO_BASE_URL}")

# --- API Endpoint ---

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Receives a conversation history and a new message, sends it to the
    LM Studio model, and returns the updated conversation history.
    """
    current_history = request.history

    # FIX: Explicitly provide all parameters to satisfy static analysis.
    # A user message never has tool calls or a tool_call_id.
    new_user_message = ChatMessage(
        role="user",
        content=request.new_message,
        tool_calls=None,
        tool_call_id=None
    )
    current_history.append(new_user_message)

    # Prepare messages for the API, ensuring they conform to the expected type.
    messages_for_api = cast(
        List[ChatCompletionMessageParam],
        [msg.model_dump(exclude_none=True) for msg in current_history]
    )

    try:
        logging.info(f"Sending {len(messages_for_api)} messages to model '{MODEL_NAME}'...")

        # The call is now simple, without the `tools` parameter. LM Studio handles it.
        completion = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages_for_api,
        )

        response_message = completion.choices[0].message
        response_dict = response_message.model_dump()
        new_assistant_message = ChatMessage.model_validate(response_dict)
        
        current_history.append(new_assistant_message)
        
        if new_assistant_message.tool_calls:
            logging.info(f"Model requested {len(new_assistant_message.tool_calls)} tool call(s).")
        else:
            logging.info("Model responded with a text message.")

        return ChatResponse(history=current_history)

    except APIConnectionError as e:
        error_detail = f"Failed to connect to LM Studio at {LM_STUDIO_BASE_URL}. Is the server running?"
        logging.error(f"{error_detail} - Error: {e}")
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=error_detail)
    except ValidationError as e:
        logging.error(f"Pydantic validation error processing model response: {e}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to validate the structure of the LLM response. Error: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An unexpected server error occurred: {str(e)}")


@app.get("/", include_in_schema=False)
async def root():
    return {"status": "ok", "message": "Chat Orchestration Layer is running."}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)